name: Build and Test Neural Search
on:
  schedule:
    - cron: '0 0 * * *'  # every night
  push:
    branches:
      - "*"
      - "feature/**"
  pull_request:
    branches:
      - "*"
      - "feature/**"

jobs:
  Get-CI-Image-Tag:
    uses: opensearch-project/opensearch-build/.github/workflows/get-ci-image-tag.yml@main
    with:
      product: opensearch

  Check-neural-search-linux:
    needs: Get-CI-Image-Tag
    strategy:
      matrix:
        java: [21, 24]
        os: [ubuntu-latest]

    name: Gradle Check Linux
    runs-on: ${{ matrix.os }}
    container:
      # using the same image which is used by opensearch-build team to build the OpenSearch Distribution
      # this image tag is subject to change as more dependencies and updates will arrive over time
      image: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-version-linux }}
      # need to switch to root so that github actions can install runner binary on container without permission issues.
      options: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-start-options }}

    steps:
      - name: Run start commands
        run: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-start-command }}

      - uses: actions/checkout@v4

      - name: Setup Java ${{ matrix.java }}
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: ${{ matrix.java }}

      - name: Run build
        run: |
          chown -R 1000:1000 `pwd`
          su `id -un 1000` -c "./gradlew check --parallel -x integTest"

      - name: Upload Coverage Report
        if: ${{ !cancelled() && contains(matrix.os, 'ubuntu') && contains(matrix.java, '21') }}
        uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

  Check-neural-search-windows:
    strategy:
      matrix:
        java: [21, 24]
        os: [windows-latest]

    name: Gradle Check Windows
    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Java ${{ matrix.java }}
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: ${{ matrix.java }}

      - name: Run build
        run: |
          ./gradlew check -x integTest

  Precommit-neural-search-linux:
    needs: Get-CI-Image-Tag
    strategy:
      matrix:
        java: [21, 24]
        os: [ubuntu-latest]

    name: Pre-commit Linux
    runs-on: ${{ matrix.os }}
    container:
      # using the same image which is used by opensearch-build team to build the OpenSearch Distribution
      # this image tag is subject to change as more dependencies and updates will arrive over time
      image: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-version-linux }}
      # need to switch to root so that github actions can install runner binary on container without permission issues.
      options: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-start-options }}

    steps:
      - name: Run start commands
        run: ${{ needs.Get-CI-Image-Tag.outputs.ci-image-start-command }}

      - uses: actions/checkout@v4

      - name: Setup Java ${{ matrix.java }}
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: ${{ matrix.java }}

      - name: Run build
        run: |
          chown -R 1000:1000 `pwd`
          su `id -un 1000` -c "./gradlew precommit -x integTest --parallel"

  integTest:
    needs: Precommit-neural-search-linux
    strategy:
      matrix:
        java: [ 21, 23 ]
        os: [ ubuntu-latest ]
    name: Integ Test JDK${{ matrix.java }}, ${{ matrix.os }}
    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK ${{ matrix.java }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ matrix.java }}
          distribution: temurin

      - name: Setup TorchServe with Model
        id: torchserve
        uses: ./.github/actions/torchserve-setup
        with:
          action: 'full-setup'
          torchserve-version: '0.9.0-cpu'
          inference-port: '8080'
          management-port: '8081'
          memory-limit: '2g'
          cpu-limit: '1'
          python-version: '3.9'
          handler-path: 'src/test/resources/remote-models/torchserve/handlers/semantic_highlighting_handler.py'
          model-name: 'semantic_highlighter'

      - name: Verify Model Deployment
        run: |
          echo "Checking loaded models..."
          curl -s http://localhost:8081/models | jq . || echo "Models: $(curl -s http://localhost:8081/models)"

          echo ""
          echo "Testing model inference..."
          TEST_RESPONSE=$(curl -s -X POST http://localhost:8080/predictions/semantic_highlighter \
            -H "Content-Type: application/json" \
            -d '{
              "question": "What is OpenSearch?",
              "context": "OpenSearch is a distributed, community-driven, Apache 2.0-licensed, 100% open-source search and analytics suite."
            }')

          echo "Response: $TEST_RESPONSE"

          if echo "$TEST_RESPONSE" | grep -q "highlights"; then
            echo "✓ Model inference working correctly!"
          else
            echo "✗ Model inference failed"
            docker logs torchserve --tail 50
            exit 1
          fi

      - name: Build and Run Integration Tests
        env:
          TORCHSERVE_ENDPOINT: ${{ steps.torchserve.outputs.torchserve-endpoint }}
          REMOTE_MODEL_ENDPOINT: ${{ steps.torchserve.outputs.model-endpoint }}
        run: |
          echo "Using TorchServe endpoint: $TORCHSERVE_ENDPOINT"
          echo "Using Remote Model endpoint: $REMOTE_MODEL_ENDPOINT"
          ./gradlew integTest --info

      - name: Cleanup TorchServe
        if: always()
        uses: ./.github/actions/torchserve-setup
        with:
          action: 'cleanup'

  integMultiNodeTest:
      needs: Precommit-neural-search-linux
      strategy:
        matrix:
          java: [ 21 ]
          os: [ ubuntu-latest ]
      name: Multi-Node Integ Test JDK${{ matrix.java }}, ${{ matrix.os }}
      runs-on: ${{ matrix.os }}
      steps:
        - uses: actions/checkout@v4

        - name: Set up JDK ${{ matrix.java }}
          uses: actions/setup-java@v4
          with:
            java-version: ${{ matrix.java }}
            distribution: temurin

        - name: Setup TorchServe with Model
          id: torchserve
          uses: ./.github/actions/torchserve-setup
          with:
            action: 'full-setup'
            torchserve-version: '0.9.0-cpu'
            inference-port: '8080'
            management-port: '8081'
            memory-limit: '2g'
            cpu-limit: '1'
            python-version: '3.9'
            handler-path: 'src/test/resources/remote-models/torchserve/handlers/semantic_highlighting_handler.py'
            model-name: 'semantic_highlighter'

        - name: Verify Model Deployment
          run: |
            echo "Checking loaded models..."
            curl -s http://localhost:8081/models | jq . || echo "Models: $(curl -s http://localhost:8081/models)"

            echo ""
            echo "Testing model inference..."
            TEST_RESPONSE=$(curl -s -X POST ${{ steps.torchserve.outputs.model-endpoint }} \
              -H "Content-Type: application/json" \
              -d '{
                "question": "What is OpenSearch?",
                "context": "OpenSearch is a distributed, community-driven, Apache 2.0-licensed, 100% open-source search and analytics suite."
              }')

            echo "Response: $TEST_RESPONSE"

            if echo "$TEST_RESPONSE" | grep -q "highlights"; then
              echo "✓ Model inference working correctly!"
            else
              echo "✗ Model inference failed"
              docker logs torchserve --tail 50
              exit 1
            fi

        - name: Build and Run Multi-Node Tests
          env:
            TORCHSERVE_ENDPOINT: ${{ steps.torchserve.outputs.torchserve-endpoint }}
            REMOTE_MODEL_ENDPOINT: ${{ steps.torchserve.outputs.model-endpoint }}
          run: |
            echo "Using TorchServe endpoint: $TORCHSERVE_ENDPOINT"
            echo "Using Remote Model endpoint: $REMOTE_MODEL_ENDPOINT"
            ./gradlew integTest -PnumNodes=3 --info

        - name: Cleanup TorchServe
          if: always()
          uses: ./.github/actions/torchserve-setup
          with:
            action: 'cleanup'
