name: 'Setup TorchServe Model'
description: 'Setup and deploy semantic highlighter model to TorchServe'
inputs:
  python-version:
    description: 'Python version to use'
    required: false
    default: '3.9'
  torchserve-endpoint:
    description: 'TorchServe endpoint URL'
    required: false
    default: 'http://localhost:8080'
  handler-path:
    description: 'Path to the TorchServe handler file'
    required: false
    default: 'src/test/resources/remote-models/torchserve/handlers/semantic_highlighting_handler.py'
  model-name:
    description: 'Name of the model to deploy'
    required: false
    default: 'semantic_highlighter'

runs:
  using: "composite"
  steps:
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install torch-model-archiver
      shell: bash
      run: |
        echo "Installing torch-model-archiver..."
        pip install -q torch-model-archiver --no-cache-dir

    - name: Wait for TorchServe
      shell: bash
      run: |
        echo "Waiting for TorchServe to be ready..."
        MAX_ATTEMPTS=30
        for i in $(seq 1 $MAX_ATTEMPTS); do
          if curl -s ${{ inputs.torchserve-endpoint }}/ping | grep -q "Healthy"; then
            echo "✓ TorchServe is ready!"
            break
          fi
          echo "Waiting for TorchServe... ($i/$MAX_ATTEMPTS)"
          [ $i -eq $MAX_ATTEMPTS ] && { echo "ERROR: TorchServe not responding"; exit 1; }
          sleep 5
        done

    - name: Setup TorchServe Container
      shell: bash
      run: |
        # Find TorchServe container
        CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=8080" | head -1)
        [ -z "$CONTAINER_ID" ] && { echo "ERROR: TorchServe container not found"; docker ps; exit 1; }
        echo "Found TorchServe container: $CONTAINER_ID"

        # Install dependencies in container
        echo "Installing dependencies in container (this may take a moment)..."
        docker exec $CONTAINER_ID pip install -q --no-cache-dir \
          transformers==4.35.0 \
          datasets \
          nltk \
          safetensors

        # Download NLTK data
        docker exec $CONTAINER_ID python -c "
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
        except:
            pass
        " 2>/dev/null
        echo "✓ Dependencies installed"

    - name: Check Model Status
      id: check-model
      shell: bash
      run: |
        # Check if model is already loaded
        # Construct management API URL (port 8081)
        MGMT_URL="http://localhost:8081"
        MODEL_STATUS=$(curl -s "${MGMT_URL}/models" 2>/dev/null || echo "[]")

        if echo "$MODEL_STATUS" | grep -q "${{ inputs.model-name }}"; then
          echo "Model '${{ inputs.model-name }}' is already loaded"
          echo "model_loaded=true" >> $GITHUB_OUTPUT
        else
          echo "Model '${{ inputs.model-name }}' not loaded"
          echo "model_loaded=false" >> $GITHUB_OUTPUT
        fi

    - name: Deploy Model
      if: steps.check-model.outputs.model_loaded == 'false'
      shell: bash
      run: |
        set -e

        # Create working directories
        mkdir -p model_store model_files

        # Prepare handler
        if [ -f "${{ inputs.handler-path }}" ]; then
          echo "Using handler from: ${{ inputs.handler-path }}"
          cp "${{ inputs.handler-path }}" handler.py
        else
          echo "ERROR: Handler file not found at ${{ inputs.handler-path }}"
          exit 1
        fi

        # Download model files from HuggingFace
        echo "Downloading model files..."
        BASE_URL="https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main"

        for file in config.json tokenizer_config.json vocab.txt; do
          curl -fsSL "$BASE_URL/$file" -o "model_files/$file"
          echo "✓ Downloaded $file"
        done

        # Download model weights
        echo "Downloading model weights (this may take a moment)..."
        curl -L --progress-bar "$BASE_URL/model.safetensors" -o "model_files/model.safetensors"
        echo "✓ Downloaded model weights"

        # Create Model Archive (MAR)
        echo "Creating model archive..."
        torch-model-archiver \
          --model-name "${{ inputs.model-name }}" \
          --version 1.0 \
          --handler handler.py \
          --extra-files model_files/ \
          --export-path model_store \
          --force

        # Deploy to TorchServe container
        CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=8080" | head -1)

        echo "Deploying model to container..."
        docker cp "model_store/${{ inputs.model-name }}.mar" "$CONTAINER_ID:/home/model-server/model-store/"

        # Create config and restart TorchServe
        cat > config.properties << EOF
        inference_address=http://0.0.0.0:8080
        management_address=http://0.0.0.0:8081
        model_store=/home/model-server/model-store
        load_models=${{ inputs.model-name }}.mar
        default_workers_per_model=1
        EOF

        docker cp config.properties "$CONTAINER_ID:/home/model-server/"

        # Restart TorchServe with new model
        echo "Restarting TorchServe..."
        docker exec $CONTAINER_ID bash -c "torchserve --stop || true"
        sleep 3
        docker exec -d $CONTAINER_ID torchserve --start \
          --model-store /home/model-server/model-store \
          --ts-config /home/model-server/config.properties

        # Wait for restart
        echo "Waiting for TorchServe to restart..."
        sleep 10

        # Re-install dependencies for workers
        echo "Installing worker dependencies..."
        docker exec $CONTAINER_ID pip install -q --no-cache-dir \
          transformers==4.35.0 datasets nltk safetensors

        docker exec $CONTAINER_ID python -c "
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
        except:
            pass
        " 2>/dev/null

    - name: Verify Model Deployment
      shell: bash
      run: |
        # Wait for model to be ready
        echo "Waiting for model to be ready..."
        MAX_ATTEMPTS=20
        MGMT_URL="http://localhost:8081"

        for i in $(seq 1 $MAX_ATTEMPTS); do
          MODEL_STATUS=$(curl -s "${MGMT_URL}/models" 2>/dev/null || echo "{}")
          if echo "$MODEL_STATUS" | grep -q "${{ inputs.model-name }}"; then
            echo "✓ Model '${{ inputs.model-name }}' is loaded"
            break
          fi
          echo "Waiting for model... ($i/$MAX_ATTEMPTS)"
          [ $i -eq $MAX_ATTEMPTS ] && { echo "ERROR: Model failed to load"; exit 1; }
          sleep 3
        done

        # Allow workers to stabilize
        sleep 5

        # Test inference
        echo "Testing model inference..."
        TEST_RESPONSE=$(curl -s -X POST "${{ inputs.torchserve-endpoint }}/predictions/${{ inputs.model-name }}" \
          -H "Content-Type: application/json" \
          -d '{"question": "What is OpenSearch?", "context": "OpenSearch is a search engine that helps you find information."}')

        if echo "$TEST_RESPONSE" | grep -q "highlights"; then
          echo "✓ Model inference successful!"
          echo "Response: $TEST_RESPONSE"
        else
          echo "ERROR: Model inference failed"
          echo "Response: $TEST_RESPONSE"

          # Debug information
          CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=8080" | head -1)
          echo "=== Debug Information ==="
          echo "Container logs (last 50 lines):"
          docker logs $CONTAINER_ID --tail 50
          exit 1
        fi
