name: 'Setup TorchServe Model'
description: 'Setup and deploy semantic highlighter model to TorchServe'
inputs:
  python-version:
    description: 'Python version to use'
    required: false
    default: '3.9'
  torchserve-endpoint:
    description: 'TorchServe endpoint URL'
    required: false
    default: 'http://localhost:8080'

runs:
  using: "composite"
  steps:
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.python-version }}

    - name: Wait for TorchServe to be ready
      shell: bash
      run: |
        echo "Waiting for TorchServe to be ready..."
        for i in {1..30}; do
          if wget -qO- ${{ inputs.torchserve-endpoint }}/ping 2>/dev/null; then
            echo "TorchServe is ready!"
            break
          fi
          echo "Waiting for TorchServe... ($i/30)"
          sleep 5
        done

    - name: Prepare and Deploy Model
      shell: bash
      run: |
        # Find the TorchServe container first
        CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=8080" | head -1)

        if [ -z "$CONTAINER_ID" ]; then
          echo "ERROR: Could not find TorchServe container on port 8080"
          echo "Available containers:"
          docker ps
          exit 1
        fi

        echo "Found TorchServe container: $CONTAINER_ID"

        # Install dependencies in the container
        echo "Installing Python dependencies in container..."
        docker exec $CONTAINER_ID pip install --no-cache-dir transformers==4.35.0 datasets nltk

        echo "Downloading NLTK data..."
        docker exec $CONTAINER_ID python -c "import nltk; nltk.download('punkt')"

        echo "✓ Dependencies installed"

        # Check if model is already loaded (from container startup)
        MODEL_CHECK=$(docker exec $CONTAINER_ID wget -qO- http://localhost:8081/models 2>/dev/null || echo "[]")
        if echo "$MODEL_CHECK" | grep -q "semantic_highlighter"; then
          echo "Model 'semantic_highlighter' is already loaded in TorchServe"

          # Test inference endpoint
          echo "Testing inference endpoint..."
          TEST_RESPONSE=$(docker exec $CONTAINER_ID wget -qO- --post-data='{"question": "What is OpenSearch?", "context": "OpenSearch is a search engine"}' \
            --header="Content-Type: application/json" \
            http://localhost:8080/predictions/semantic_highlighter 2>/dev/null || echo "{}")

          if echo "$TEST_RESPONSE" | grep -q "highlights"; then
            echo "✓ Model is working correctly!"
            echo "Response: $TEST_RESPONSE"
            exit 0
          else
            echo "Model is loaded but not responding correctly"
            echo "Response: $TEST_RESPONSE"
          fi
        fi

        echo "Model not loaded, setting it up..."

        # Install required Python packages (only what's needed for MAR creation)
        pip install torch-model-archiver --no-cache-dir

        # Create directories
        mkdir -p model_store model_files

        # Check current directory and list files
        echo "Current directory: $(pwd)"
        echo "Looking for handler file..."
        
        # Try to find the handler file
        HANDLER_PATH="src/test/resources/remote-models/torchserve/handlers/torchserve_handler.py"
        
        if [ -f "$HANDLER_PATH" ]; then
          echo "Found handler at: $HANDLER_PATH"
          cp "$HANDLER_PATH" handler.py
        else
          echo "Handler not found at $HANDLER_PATH, checking repository structure..."
          ls -la src/test/resources/remote-models/torchserve/handlers/ 2>/dev/null || echo "Directory not found"
          
          # Try alternative paths
          if [ -f "neural-search/$HANDLER_PATH" ]; then
            echo "Found handler at neural-search/$HANDLER_PATH"
            cp "neural-search/$HANDLER_PATH" handler.py
          else
            echo "ERROR: Could not find handler file in expected locations"
            echo "Repository structure:"
            find . -name "torchserve_handler.py" -type f 2>/dev/null | head -5
            exit 1
          fi
        fi

        if [ ! -f "handler.py" ]; then
          echo "ERROR: Failed to copy handler file"
          exit 1
        fi
        
        echo "✓ Handler file copied successfully"

        # Download model files from HuggingFace
        echo "Downloading model files from HuggingFace..."

        curl -fsSL https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main/config.json -o model_files/config.json || exit 1
        echo "✓ config.json"

        curl -fsSL https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main/tokenizer_config.json -o model_files/tokenizer_config.json || exit 1
        echo "✓ tokenizer_config.json"

        curl -fsSL https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main/vocab.txt -o model_files/vocab.txt || exit 1
        echo "✓ vocab.txt"

        # Download the large model file
        echo "Downloading model weights..."
        curl -L --progress-bar https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main/model.safetensors -o model_files/model.safetensors || exit 1
        echo "✓ model.safetensors"

        # Create MAR file
        echo "Creating model archive..."
        torch-model-archiver \
          --model-name semantic_highlighter \
          --version 1.0 \
          --handler handler.py \
          --extra-files model_files/ \
          --export-path model_store \
          --force

        # Copy MAR file to container
        echo "Copying MAR to container..."
        docker cp model_store/semantic_highlighter.mar $CONTAINER_ID:/home/model-server/model-store/

        # Restart TorchServe with the model
        echo "Restarting TorchServe with model..."
        docker restart $CONTAINER_ID

        # Wait for TorchServe to be ready
        echo "Waiting for TorchServe to restart..."
        for i in {1..30}; do
          if curl -s http://localhost:8080/ping | grep -q "Healthy"; then
            echo "TorchServe is ready!"
            break
          fi
          echo -n "."
          sleep 2
        done
        echo ""

        # Register the model with TorchServe
        echo "Registering model with TorchServe..."
        REGISTER_RESPONSE=$(curl -s -X POST "http://localhost:8081/models" \
          -F "model_name=semantic_highlighter" \
          -F "url=semantic_highlighter.mar" \
          -F "initial_workers=1" \
          -F "synchronous=true" \
          -F "max_batch_delay=50")
        
        echo "Register response: $REGISTER_RESPONSE"
        
        # Wait for model to be ready
        echo "Waiting for model to be ready..."
        for i in {1..30}; do
          MODEL_STATUS=$(curl -s http://localhost:8081/models/semantic_highlighter | jq -r '.[] | select(.modelName=="semantic_highlighter") | .workers[0].status' 2>/dev/null || echo "")
          if [ "$MODEL_STATUS" = "READY" ]; then
            echo "Model is ready!"
            break
          fi
          echo "Model status: $MODEL_STATUS (attempt $i/30)"
          sleep 3
        done

        # Verify model is loaded
        echo "Verifying model is loaded..."
        MODEL_CHECK=$(curl -s http://localhost:8081/models)
        echo "Models loaded: $MODEL_CHECK"

        # Test inference
        echo "Testing inference..."
        TEST_RESPONSE=$(curl -s -X POST http://localhost:8080/predictions/semantic_highlighter \
          -H "Content-Type: application/json" \
          -d '{"question": "What is OpenSearch?", "context": "OpenSearch is a search engine"}' || echo "{}")

        if echo "$TEST_RESPONSE" | grep -q "tokens"; then
          echo "✓ TorchServe model deployment successful!"
          echo "Response: $TEST_RESPONSE"
        else
          echo "ERROR: Model not responding correctly"
          echo "Response: $TEST_RESPONSE"
          docker logs $CONTAINER_ID --tail 50
          exit 1
        fi
