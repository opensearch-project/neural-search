name: 'TorchServe Complete Setup'
description: 'Complete TorchServe setup including container start, model deployment, and optional cleanup'

inputs:
  action:
    description: 'Action to perform: start, setup-model, cleanup, or full-setup (default)'
    required: false
    default: 'full-setup'
  torchserve-version:
    description: 'TorchServe Docker image version'
    required: false
    default: '0.9.0-cpu'
  inference-port:
    description: 'TorchServe inference port'
    required: false
    default: '8080'
  management-port:
    description: 'TorchServe management port'
    required: false
    default: '8081'
  memory-limit:
    description: 'Docker memory limit'
    required: false
    default: '2g'
  cpu-limit:
    description: 'Docker CPU limit'
    required: false
    default: '1'
  python-version:
    description: 'Python version for model archiver'
    required: false
    default: '3.9'
  handler-path:
    description: 'Path to the TorchServe handler file'
    required: false
    default: 'src/test/resources/remote-models/torchserve/handlers/semantic_highlighting_handler.py'
  model-name:
    description: 'Name of the model to deploy'
    required: false
    default: 'semantic_highlighter'

outputs:
  torchserve-endpoint:
    description: 'TorchServe inference endpoint URL'
    value: ${{ steps.set-outputs.outputs.torchserve-endpoint }}
  management-endpoint:
    description: 'TorchServe management endpoint URL'
    value: ${{ steps.set-outputs.outputs.management-endpoint }}
  model-endpoint:
    description: 'Semantic highlighter model endpoint URL'
    value: ${{ steps.set-outputs.outputs.model-endpoint }}

runs:
  using: "composite"
  steps:
    # Set outputs that are always available
    - name: Set Outputs
      id: set-outputs
      shell: bash
      run: |
        echo "torchserve-endpoint=http://localhost:${{ inputs.inference-port }}" >> $GITHUB_OUTPUT
        echo "management-endpoint=http://localhost:${{ inputs.management-port }}" >> $GITHUB_OUTPUT
        echo "model-endpoint=http://localhost:${{ inputs.inference-port }}/predictions/${{ inputs.model-name }}" >> $GITHUB_OUTPUT

    # Start TorchServe Container (for 'start' and 'full-setup' actions)
    - name: Start TorchServe Container
      if: inputs.action == 'start' || inputs.action == 'full-setup'
      shell: bash
      run: |
        echo "Starting TorchServe container..."
        docker run -d \
          --name torchserve \
          -p ${{ inputs.inference-port }}:8080 \
          -p ${{ inputs.management-port }}:8081 \
          --memory ${{ inputs.memory-limit }} \
          --cpus ${{ inputs.cpu-limit }} \
          pytorch/torchserve:${{ inputs.torchserve-version }} \
          torchserve --start --model-store /home/model-server/model-store --ts-config /home/model-server/config.properties
        
        # Give container time to start
        sleep 10
        
        echo "Container status:"
        docker ps
        
        echo "Checking TorchServe health..."
        for i in {1..10}; do
          if curl -s http://localhost:${{ inputs.inference-port }}/ping | grep -q "Healthy"; then
            echo "✓ TorchServe is ready!"
            break
          fi
          echo "Waiting for TorchServe... ($i/10)"
          [ $i -eq 10 ] && { echo "ERROR: TorchServe not responding"; exit 1; }
          sleep 3
        done

    # Setup Model (for 'setup-model' and 'full-setup' actions)
    - name: Setup Python
      if: inputs.action == 'setup-model' || inputs.action == 'full-setup'
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install torch-model-archiver
      if: inputs.action == 'setup-model' || inputs.action == 'full-setup'
      shell: bash
      run: |
        echo "Installing torch-model-archiver..."
        pip install -q torch-model-archiver --no-cache-dir

    - name: Setup TorchServe Container Dependencies
      if: inputs.action == 'setup-model' || inputs.action == 'full-setup'
      shell: bash
      run: |
        # Wait for TorchServe
        echo "Waiting for TorchServe to be ready..."
        MAX_ATTEMPTS=30
        for i in $(seq 1 $MAX_ATTEMPTS); do
          if curl -s http://localhost:${{ inputs.inference-port }}/ping | grep -q "Healthy"; then
            echo "✓ TorchServe is ready!"
            break
          fi
          echo "Waiting for TorchServe... ($i/$MAX_ATTEMPTS)"
          [ $i -eq $MAX_ATTEMPTS ] && { echo "ERROR: TorchServe not responding"; exit 1; }
          sleep 5
        done

        # Find TorchServe container
        CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=${{ inputs.inference-port }}" | head -1)
        [ -z "$CONTAINER_ID" ] && { echo "ERROR: TorchServe container not found"; docker ps; exit 1; }
        echo "Found TorchServe container: $CONTAINER_ID"

        # Install dependencies in container
        echo "Installing dependencies in container (this may take a moment)..."
        docker exec $CONTAINER_ID pip install -q --no-cache-dir \
          transformers==4.35.0 \
          datasets \
          nltk \
          safetensors

        # Download NLTK data
        docker exec $CONTAINER_ID python -c "
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
        except:
            pass
        " 2>/dev/null
        echo "✓ Dependencies installed"

    - name: Check Model Status
      id: check-model
      if: inputs.action == 'setup-model' || inputs.action == 'full-setup'
      shell: bash
      run: |
        # Check if model is already loaded
        MGMT_URL="http://localhost:${{ inputs.management-port }}"
        MODEL_STATUS=$(curl -s "${MGMT_URL}/models" 2>/dev/null || echo "[]")

        if echo "$MODEL_STATUS" | grep -q "${{ inputs.model-name }}"; then
          echo "Model '${{ inputs.model-name }}' is already loaded"
          echo "model_loaded=true" >> $GITHUB_OUTPUT
        else
          echo "Model '${{ inputs.model-name }}' not loaded"
          echo "model_loaded=false" >> $GITHUB_OUTPUT
        fi

    - name: Deploy Model
      if: (inputs.action == 'setup-model' || inputs.action == 'full-setup') && steps.check-model.outputs.model_loaded == 'false'
      shell: bash
      run: |
        set -e

        # Create working directories
        mkdir -p model_store model_files

        # Prepare handler
        if [ -f "${{ inputs.handler-path }}" ]; then
          echo "Using handler from: ${{ inputs.handler-path }}"
          cp "${{ inputs.handler-path }}" handler.py
        else
          echo "ERROR: Handler file not found at ${{ inputs.handler-path }}"
          exit 1
        fi

        # Download model files from HuggingFace
        echo "Downloading model files..."
        BASE_URL="https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1/resolve/main"

        for file in config.json tokenizer_config.json vocab.txt; do
          curl -fsSL "$BASE_URL/$file" -o "model_files/$file"
          echo "✓ Downloaded $file"
        done

        # Download model weights
        echo "Downloading model weights (this may take a moment)..."
        curl -L --progress-bar "$BASE_URL/model.safetensors" -o "model_files/model.safetensors"
        echo "✓ Downloaded model weights"

        # Create Model Archive (MAR)
        echo "Creating model archive..."
        torch-model-archiver \
          --model-name "${{ inputs.model-name }}" \
          --version 1.0 \
          --handler handler.py \
          --extra-files model_files/ \
          --export-path model_store \
          --force

        # Deploy to TorchServe container
        CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=${{ inputs.inference-port }}" | head -1)

        echo "Deploying model to container..."
        docker cp "model_store/${{ inputs.model-name }}.mar" "$CONTAINER_ID:/home/model-server/model-store/"

        # Create config and restart TorchServe
        cat > config.properties << EOF
        inference_address=http://0.0.0.0:8080
        management_address=http://0.0.0.0:8081
        model_store=/home/model-server/model-store
        load_models=${{ inputs.model-name }}.mar
        default_workers_per_model=1
        EOF

        docker cp config.properties "$CONTAINER_ID:/home/model-server/"

        # Restart TorchServe with new model
        echo "Restarting TorchServe..."
        docker exec $CONTAINER_ID bash -c "torchserve --stop || true"
        sleep 3
        docker exec -d $CONTAINER_ID torchserve --start \
          --model-store /home/model-server/model-store \
          --ts-config /home/model-server/config.properties

        # Wait for restart
        echo "Waiting for TorchServe to restart..."
        sleep 10

        # Re-install dependencies for workers
        echo "Installing worker dependencies..."
        docker exec $CONTAINER_ID pip install -q --no-cache-dir \
          transformers==4.35.0 datasets nltk safetensors

        docker exec $CONTAINER_ID python -c "
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
        except:
            pass
        " 2>/dev/null

    - name: Verify Model Deployment
      if: inputs.action == 'setup-model' || inputs.action == 'full-setup'
      shell: bash
      run: |
        # Wait for model to be ready
        echo "Waiting for model to be ready..."
        MAX_ATTEMPTS=20
        MGMT_URL="http://localhost:${{ inputs.management-port }}"

        for i in $(seq 1 $MAX_ATTEMPTS); do
          MODEL_STATUS=$(curl -s "${MGMT_URL}/models" 2>/dev/null || echo "{}")
          if echo "$MODEL_STATUS" | grep -q "${{ inputs.model-name }}"; then
            echo "✓ Model '${{ inputs.model-name }}' is loaded"
            break
          fi
          echo "Waiting for model... ($i/$MAX_ATTEMPTS)"
          [ $i -eq $MAX_ATTEMPTS ] && { echo "ERROR: Model failed to load"; exit 1; }
          sleep 3
        done

        # Allow workers to stabilize
        sleep 5

        # Test inference
        echo "Testing model inference..."
        TEST_RESPONSE=$(curl -s -X POST "http://localhost:${{ inputs.inference-port }}/predictions/${{ inputs.model-name }}" \
          -H "Content-Type: application/json" \
          -d '{"question": "What is OpenSearch?", "context": "OpenSearch is a search engine that helps you find information."}')

        if echo "$TEST_RESPONSE" | grep -q "highlights"; then
          echo "✓ Model inference successful!"
          echo "Response: $TEST_RESPONSE"
        else
          echo "ERROR: Model inference failed"
          echo "Response: $TEST_RESPONSE"

          # Debug information
          CONTAINER_ID=$(docker ps --format "{{.ID}}" --filter "publish=${{ inputs.inference-port }}" | head -1)
          echo "=== Debug Information ==="
          echo "Container logs (last 50 lines):"
          docker logs $CONTAINER_ID --tail 50
          exit 1
        fi

    # Cleanup (for 'cleanup' action only)
    - name: Cleanup TorchServe
      if: inputs.action == 'cleanup'
      shell: bash
      run: |
        echo "Cleaning up TorchServe container..."
        docker stop torchserve || true
        docker rm torchserve || true
        echo "✓ TorchServe container cleaned up"